{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ruGPT3 Finetuning\n",
        "\n",
        "Попробуем натренировать сеть ruGPT3 на архивах старой компьютерной сети FIDONet.\n",
        "\n",
        "Для начала, скачаем датасет (подготовлен [Дмитрием Сошниковым](https://soshnikov.com)):"
      ],
      "metadata": {
        "id": "FgBA0VpHL9B9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e0YhEfDKV4H",
        "outputId": "57898c26-c52c-4457-fbcd-fcc5ade8a8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  426M  100  426M    0     0  19.7M      0  0:00:21  0:00:21 --:--:-- 31.2M\n"
          ]
        }
      ],
      "source": [
        "!curl -L https://storage.yandexcloud.net/mypub/data/train.txt.gz | gzip -d > train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на то, как выглядит датасет:"
      ],
      "metadata": {
        "id": "BdrwGQdQPt-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMUMphLBPyJh",
        "outputId": "d27f9a56-0c30-45be-f94e-000c1362a707"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "From: Andrey Chekalin <Andrey.Chekalin@p20.f57.n5022.z2.fidonet.org>\n",
            "Date: Mon, 07 Jul 2003 23:13:14 +0400\n",
            "Subject: test\n",
            "Organization: -= Alien Station =-\n",
            "Lines: 7\n",
            "\n",
            "                          Приветствую Вас, All!\n",
            "\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для ускорения возьмём часть датасета. Для полноценного обучения лучше брать весь."
      ],
      "metadata": {
        "id": "wPyw-MdmRTro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -1000000 train.txt > train_sub.txt"
      ],
      "metadata": {
        "id": "jirmXa-TRZMo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим последние версии библиотек:"
      ],
      "metadata": {
        "id": "EPzeyYNeQNlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate evaluate datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE4HWWTdQQfi",
        "outputId": "78879997-20db-49ad-fd6b-20e2b9aa39be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.1 datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на версию библиотеки transformers:"
      ],
      "metadata": {
        "id": "BWb2xcPzO0k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEDSp5MhOrEU",
        "outputId": "e895d538-d265-470a-c880-93cb2a7274a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers==4.41.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачаем скрипт для тренировки сети для соответствующей версии (ниже пример для версии 4.41.2):"
      ],
      "metadata": {
        "id": "L8BDO3-nO74r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/huggingface/transformers/ab0f050b42d903f34d6eb97f3f8c0c07f0517ad2/examples/pytorch/language-modeling/run_clm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nc6NYuZO5x-",
        "outputId": "10089a56-5955-4f80-ab1d-d028b4a0e568"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 28234  100 28234    0     0  87876      0 --:--:-- --:--:-- --:--:-- 87683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запускаем обучение:"
      ],
      "metadata": {
        "id": "xGcKlxv_PrTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "    --train_file train_sub.txt \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --block_size 2048 \\\n",
        "    --dataset_config_name plain_text \\\n",
        "    --do_train \\\n",
        "    --output_dir=models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCf6Xb8pPmNK",
        "outputId": "a5b29bef-b8d9-456f-ae8a-f0c1fa533467"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-05 19:56:44.157971: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-05 19:56:44.158026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-05 19:56:44.275382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-05 19:56:44.518356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-05 19:56:46.644060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/05/2024 19:56:51 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/05/2024 19:56:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/runs/Jul05_19-56-51_f79b74e521fa,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=models,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-bf535fe7f70e550f\n",
            "07/05/2024 19:56:51 - INFO - datasets.builder - Using custom data configuration default-bf535fe7f70e550f\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "07/05/2024 19:56:51 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "Generating dataset text (/root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc)\n",
            "07/05/2024 19:56:51 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc...\n",
            "07/05/2024 19:56:51 - INFO - datasets.builder - Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc...\n",
            "Downloading took 0.0 min\n",
            "07/05/2024 19:56:51 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "07/05/2024 19:56:51 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "07/05/2024 19:56:51 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 1000000 examples [00:01, 898977.78 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "07/05/2024 19:56:52 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc. Subsequent calls will reuse this data.\n",
            "07/05/2024 19:56:52 - INFO - datasets.builder - Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc. Subsequent calls will reuse this data.\n",
            "Using custom data configuration default-bf535fe7f70e550f\n",
            "07/05/2024 19:56:52 - INFO - datasets.builder - Using custom data configuration default-bf535fe7f70e550f\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "07/05/2024 19:56:52 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "07/05/2024 19:56:52 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "07/05/2024 19:56:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "Found cached dataset text (/root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc)\n",
            "07/05/2024 19:56:52 - INFO - datasets.builder - Found cached dataset text (/root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "07/05/2024 19:56:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "Using custom data configuration default-bf535fe7f70e550f\n",
            "07/05/2024 19:56:53 - INFO - datasets.builder - Using custom data configuration default-bf535fe7f70e550f\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "07/05/2024 19:56:53 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "07/05/2024 19:56:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "07/05/2024 19:56:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "Found cached dataset text (/root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc)\n",
            "07/05/2024 19:56:53 - INFO - datasets.builder - Found cached dataset text (/root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "07/05/2024 19:56:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 720/720 [00:00<00:00, 3.09MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-05 19:56:53,595 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-05 19:56:53,597 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 1.25k/1.25k [00:00<00:00, 6.16MB/s]\n",
            "vocab.json: 100% 1.71M/1.71M [00:00<00:00, 4.13MB/s]\n",
            "merges.txt: 100% 1.27M/1.27M [00:00<00:00, 3.79MB/s]\n",
            "special_tokens_map.json: 100% 574/574 [00:00<00:00, 2.94MB/s]\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-05 19:56:56,174 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-05 19:56:56,175 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-05 19:56:56,175 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-05 19:56:56,175 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-05 19:56:56,175 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-05 19:56:56,175 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/tokenizer_config.json\n",
            "pytorch_model.bin: 100% 551M/551M [00:04<00:00, 134MB/s]\n",
            "[INFO|modeling_utils.py:3474] 2024-07-05 19:57:01,351 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sberbank-ai--rugpt3small_based_on_gpt2/snapshots/a9307e696cd3c5b7f953ff4cb19d76a4d81821d5/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:962] 2024-07-05 19:57:02,003 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4280] 2024-07-05 19:57:02,985 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-05 19:57:02,985 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|modeling_utils.py:3797] 2024-07-05 19:57:03,183 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on dataset:   0% 0/950000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-1be0b5c597792fe6.arrow\n",
            "07/05/2024 19:57:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-1be0b5c597792fe6.arrow\n",
            "Running tokenizer on dataset: 100% 950000/950000 [00:54<00:00, 17284.49 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-49a0913ec24c4af7.arrow\n",
            "07/05/2024 19:57:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-49a0913ec24c4af7.arrow\n",
            "Running tokenizer on dataset: 100% 50000/50000 [00:02<00:00, 20902.91 examples/s]\n",
            "Grouping texts in chunks of 2048:   0% 0/950000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-3aabf9a5d159d3e7.arrow\n",
            "07/05/2024 19:58:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-3aabf9a5d159d3e7.arrow\n",
            "Grouping texts in chunks of 2048: 100% 950000/950000 [00:33<00:00, 28092.33 examples/s]\n",
            "Grouping texts in chunks of 2048:   0% 0/50000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-f367e0bfdfe225fb.arrow\n",
            "07/05/2024 19:58:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-bf535fe7f70e550f/0.0.0/73c2faa07573e39dca89c9d5cbd568526dc2e15125e5a097f5f512be735457bc/cache-f367e0bfdfe225fb.arrow\n",
            "Grouping texts in chunks of 2048: 100% 50000/50000 [00:02<00:00, 23784.45 examples/s]\n",
            "[INFO|trainer.py:2078] 2024-07-05 19:58:37,776 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-05 19:58:37,776 >>   Num examples = 7,460\n",
            "[INFO|trainer.py:2080] 2024-07-05 19:58:37,776 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2081] 2024-07-05 19:58:37,776 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2084] 2024-07-05 19:58:37,776 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:2085] 2024-07-05 19:58:37,776 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2086] 2024-07-05 19:58:37,776 >>   Total optimization steps = 7,460\n",
            "[INFO|trainer.py:2087] 2024-07-05 19:58:37,777 >>   Number of trainable parameters = 125,231,616\n",
            "{'loss': 1.0182, 'grad_norm': 1.9472070932388306, 'learning_rate': 4.664879356568364e-05, 'epoch': 0.07}\n",
            "  7% 500/7460 [07:31<1:45:44,  1.10it/s][INFO|trainer.py:3410] 2024-07-05 20:06:09,167 >> Saving model checkpoint to models/checkpoint-500\n",
            "[INFO|configuration_utils.py:472] 2024-07-05 20:06:09,169 >> Configuration saved in models/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-05 20:06:09,169 >> Configuration saved in models/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:2618] 2024-07-05 20:06:15,456 >> Model weights saved in models/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-05 20:06:15,457 >> tokenizer config file saved in models/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-05 20:06:15,457 >> Special tokens file saved in models/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.9389, 'grad_norm': 1.4169327020645142, 'learning_rate': 4.32975871313673e-05, 'epoch': 0.13}\n",
            " 13% 1000/7460 [15:17<1:38:04,  1.10it/s][INFO|trainer.py:3410] 2024-07-05 20:13:55,043 >> Saving model checkpoint to models/checkpoint-1000\n",
            "[INFO|configuration_utils.py:472] 2024-07-05 20:13:55,045 >> Configuration saved in models/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-05 20:13:55,046 >> Configuration saved in models/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:2618] 2024-07-05 20:14:05,161 >> Model weights saved in models/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-05 20:14:05,162 >> tokenizer config file saved in models/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-05 20:14:05,162 >> Special tokens file saved in models/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.8728, 'grad_norm': 1.1405532360076904, 'learning_rate': 3.994638069705094e-05, 'epoch': 0.2}\n",
            " 20% 1500/7460 [23:08<1:30:15,  1.10it/s][INFO|trainer.py:3410] 2024-07-05 20:21:46,199 >> Saving model checkpoint to models/checkpoint-1500\n",
            "[INFO|configuration_utils.py:472] 2024-07-05 20:21:46,200 >> Configuration saved in models/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-05 20:21:46,201 >> Configuration saved in models/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:2618] 2024-07-05 20:21:53,229 >> Model weights saved in models/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-05 20:21:53,230 >> tokenizer config file saved in models/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-05 20:21:53,230 >> Special tokens file saved in models/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.8863, 'grad_norm': 0.9800612330436707, 'learning_rate': 3.659517426273459e-05, 'epoch': 0.27}\n",
            " 27% 2000/7460 [30:56<1:22:47,  1.10it/s][INFO|trainer.py:3410] 2024-07-05 20:29:34,452 >> Saving model checkpoint to models/checkpoint-2000\n",
            "[INFO|configuration_utils.py:472] 2024-07-05 20:29:34,453 >> Configuration saved in models/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-05 20:29:34,454 >> Configuration saved in models/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:2618] 2024-07-05 20:29:37,857 >> Model weights saved in models/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-05 20:29:37,858 >> tokenizer config file saved in models/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-05 20:29:37,858 >> Special tokens file saved in models/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.8673, 'grad_norm': 0.7713437676429749, 'learning_rate': 3.324396782841823e-05, 'epoch': 0.34}\n",
            " 34% 2500/7460 [38:48<1:15:05,  1.10it/s][INFO|trainer.py:3410] 2024-07-05 20:37:26,389 >> Saving model checkpoint to models/checkpoint-2500\n",
            "[INFO|configuration_utils.py:472] 2024-07-05 20:37:26,391 >> Configuration saved in models/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-05 20:37:26,391 >> Configuration saved in models/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:2618] 2024-07-05 20:37:39,068 >> Model weights saved in models/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-05 20:37:39,069 >> tokenizer config file saved in models/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-05 20:37:39,070 >> Special tokens file saved in models/checkpoint-2500/special_tokens_map.json\n",
            " 36% 2720/7460 [42:39<1:11:59,  1.10it/s]Traceback (most recent call last):\n",
            "  File \"/content/run_clm.py\", line 654, in <module>\n",
            "    main()\n",
            "  File \"/content/run_clm.py\", line 602, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1885, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2221, in _inner_training_loop\n",
            "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
            "KeyboardInterrupt\n",
            " 36% 2720/7460 [42:41<1:14:23,  1.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем генерацию:"
      ],
      "metadata": {
        "id": "eS1xD62Vj35g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM,AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = 'models/checkpoint-2500'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "pipe = pipeline(model=model,tokenizer=tokenizer,task=\"text-generation\",device=\"cuda\")"
      ],
      "metadata": {
        "id": "iALnxBY8QKXD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(\"<s>Area: ru.anekdot\",do_sample=True,max_length=1500)[0]['generated_text'].replace('\\\\n','\\n')\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "o18Fen0bSeub",
        "outputId": "319f9a8f-32d2-43bc-c22e-ed0417ff4ee1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>Area: ru.anekdot.ru, 28/июл/2009, 7\\nOrganization: U$A station\\nLines: 10\\nBytes: 1924\\nX-Original-Bytes: 1914\\n\\n╔═[■]═════════════════════════[ Hello All! ]═══════────────--─--─-\\n\\nБеда. Отказавшись от всех денег, он отказался от всего,\\nчто у него есть. До самого самого конца не смог понять, как он выглядит\\nдо этого момента.\\n\\t\\t\\n\\n╚═[ Andrew Usachov ]═════════════════[ 28 июл 2009, 01:41 ]══───-─-\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dl5MXe_VomLx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}